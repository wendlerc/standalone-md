# virtual experience designer;

Dear Google Deepmind Hiring Team,

I am writing to apply for the "Research Scientist, Generative Worlds" role. I am a postdoctoral researcher in David Bau's interpretable deep learning lab. In our lab we ask how deep neural networks achieve their remarkable capabilities, mechanistically, and whether their internal computations can be made legible to humans. That said, to me generative world models like Genie 3 and video models like VEO are by far the most remarkable and most interesting development of our time. With their Genie 3, Jack Parker-Holder, Shlomi Fruchter and the rest of the Genie 3 team truely gifted us with a glimpse into the future and added another entry to Deepmind's ever growing list of paradigm shifting contributions (DQN, AlphaGo, AlphaZero, AlphaFold2, AlphaTensor) and a milestone to Deepmind's journey towards AI. 

It is completely obvious to me that this research direction of generative world modelling using deep neural networks (today transformers, tomorrow maybe something else) offers insights and treasures beyond our current imagination. This entire research space currently lives at the boundary of what's possible and in addition to large language model / agent research presents the second frontier of AI. This technology is almost like an entirely new medium and has the potential to revolutionize many different verticals all at once. For example, consider the training of surgeons, who today have to go through an expensive training process with many steps building up their skills before they can perform their first real surgery. I imagine the two steps most similar to their real target surgery are the human-cadaver based simulation of the target surgery and watching more experienced colleagues perform this surgery. In many parts of the world (parts of Asia, Africa, Eastern Europe) cadaver donations don't meet the requirements of medical doctor training programs. Five years from now, in a world in which generative world models excell, surgeons could train for their upcoming surgery they have to perform inside of a VR experience powered by our world model. Five years from now is not a long time and the simulation must be very accurate for the simulated experience to be helpful for the young surgeon, thus, to make sure the experience is accurate and instructive we can add another real-human participant to the experience, the senior surgeon with lots of experience. Almost like in a video game, the to-be-trained surgeon is immersed in their simulated surgery as a "player 1", while the senior surgeon participates, maybe as an additional "player 2" with the privilege to modify the simulated content towards medical accuracy or as a "game master" ensuring medical accuracy but also that key lessons are learned while staying invisible to "player 1". Or maybe models improve so dramatically that the senior surgeon can effortlessly create such experiences for all sorts of surgeries and novices train in them asynchronously without the need of real-time interventions by the senior. I picked the surgeon training example, because it is easy to imagine that a program like this can make sense not only as a vision, but also from an economical point of view. However, this setup is much more general and will be useful across many different verticals. E.g., it will revolutionize how psychotherapists can treat their patients. In Denmark there is already today a company called HEKA VR [1], which investigate whether VR headsets can improve treatment of schizophrenia. They do this by manually (using a team of technical experts) creating avatars and voice modulations to mimick the voices perceived by the shizophrenic patient. This allows the therapists to take the role of these avatars. The goal of the therapy is to shift power dynamics between the patient and their percieved voices and along the way the therapist also replaces negative things the voices say with more positive ones. Also it is easy to come up with other amazing things that will be enabled by such a technology, some of which are listed in the job post (e.g., enabling top-tier education to children independent from their geographical location and living conditions; training robots more cost-effectively in distilled-, fast world models; creative applications with this new medium by empowering artists; just to name a few).

The blocker for the envisioned scenario is neither the lack of computational resources, nor, the level of quality of VR experiences, nor, the lack of people creating and participating in such experiences. The only thing seperating us from this arguably better world are (pun intended) better world models. World models capable of realistic, consistent, long time horizion simulations that can support multiple participants (humans and agents with potentially vastly different roles as described above) and that makes it seamless to design and customize simulated worlds. The consistency and long time horizon aspects maybe can already be tackled to some extend with today's long-context architectures that leverage learned linear memory blocks such as XLSTM, RWKV, or Deltanet. If these long-context architectures are too hard to train from scratch maybe they can be instantiated from a transformer-based big world model and small adaptations of today's diffusion transformer distiallation techniques such as DMD/CausVid/Self-Force. Multi-agent world models probaby require innovations in the way we pre- and post-train world models. Maybe a post-training step comparable to instruction-tuning with a diverse enough dataset curated using human data from humans in shared experiences or multiple simulated agents can make progress towards something like that, or maybe this requires rethinking the architecture a bit. Lastly, while from what we can observe on text2image models there are already many promising customization techniques, the simpler ones with low enough barriers to be accessible to non-technical users don't seem to be accurate enough and the more involved ones that are capable of better results are not accessible to non-technical users. Maybe scaling the models will get us there or maybe some interpretability inspired customization techniques.  

Talk about my fit:...


with appropriate data containing actions and multiple simulated agents



that allow for indefinitely long simulations with world-consistency, world models that allow a simulation consistent for multiple human and/or argent participants, world models that 

innovations in long-context and learned memory systems (e.g., XLSTM, delta-net, and friends)


[1] HEKA VR, https://hekavr.com/research/



 and therapists can take the role of these avatars

Using their setup, they can already today allow patients 

T to replace the usual mallicious messages from these voices with beneficial ones. Additionally, they also can leverage this setting to shift the power dynamics and empower the patients to  



 a simulation of their upcoming 



 step most similar to the real surgery


It is obvious to me that ... applications: training doctors in simulations, training biologists without animal experiments, democratizing access to education imagine a child independent of where they are born being able to learn whatever humanity has to offer by entering Genie5.




This already brings me to you, since the Genie 3 post, I am captivated by this technology and the future it might hold for us. 



To me, generative world models are by far the most remarkable and most interesting development of our time. Jack Parker-Holder, Shlomi Fruchter and the rest of the Genie 3 team 


with their remarkable execution of Genie 3 have truely shown us a glimpse into the future that I cannot ignore.




To contribute deep learning in this role is t




Much of our research falls within the emerging field of mechanistic interpretability. While most of the field is mostly focused on large language models and their derivates (chat & reasoning models), I am the most excited about extending the field towards text to image models, video and world models. That said I am a deep learning researcher by heart and to me Genie 3 is the by far most interesting development in our field. With their remarkable execution on Genie 3, Jack Parker-Holder, Shlomi Fruchter and their team with their remarkable execution of Genie 3 have truely shown us a glimpse into the future that I cannot ignore.





In my role as a postdoc I am fortunate to advise many brilliant students


 My projects span many different architectures (large language models, reasoning models, text-to-image models, protein folding models) and different research questions (how do capabilities form during pretraining)


My projects (executed by myself and many that I am advising) 
he projects I executing and advising sp

my projects in which I advis

and the advising function that it enables me to perform my projects

As a deep learning researcher, to me Genie 3 is the by far most interesting development in our field. With their remarkable execution on Genie 3, Jack Parker-Holder, Shlomi Fruchter and their team with their remarkable execution of Genie 3 have shown us a glimpse into the future that I cannot ignore.



have shown us a glimpse into the future.

Since the blogpost in August, I have not been able to stop thinking about it. I was familiar with Genie already before and have been talking to world-model start ups. However, 



Genie 3 with its remarkable execution 

 



I am currently a postdoctoral researcher in David Bau's interpretable deep learning lab. In our lab, we ask how deep neural networks trained with gradient descent work mechanistically. 

As a deep learning researcher 
Genie 3 



I am very excited about this job opening, because Genie 3 is in my opinion by far the most interesting 

Vision & how to get there

Background

Why GDM
