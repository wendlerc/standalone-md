Chris Wendler
Dear Google DeepMind Hiring Team, 
I am writing to apply for the Research Scientist, Generative Worlds role. I am a postdoctoral researcher in David Bau’s interpretable deep learning lab at Northeastern University and previously in Robert West’s data science lab at EPFL Lausanne. Across these roles, my work has focused on understanding and controlling large generative models, and I am eager to bring that experience to the frontier of generative world modeling at Google DeepMind. World models like Genie and Veo, to me, represent the second great frontier of AI alongside agentic systems. Innovating in this space living right at the boundary of what’s possible deeply resonates with my core motivation of scientific curiosity.
Background. This direction naturally connects to my work on text-to-image models, recently culminating in our NeurIPS 2025 accepted paper [1] with my brilliant students Viacheslav Surkov, who initiated the direction, and Antonio Mari, who advanced it significantly. In this work, we extended sparse autoencoders (SAEs) to the visual stream of state-of-the-art text-to-image models such as SDXL and FLUX. SAEs learn interpretable features using the autoencoding objective on a large dataset of, in our case, updates performed by a hidden layer to a spatial location in the visual stream. The resulting features are interpretable and many of them act almost like brush tools that can controllably affect image generation within spatial regions depending on their strength. In spirit, this form of non-linguistic control parallels the learned latent actions used to guide video generation in Genie 1. The visual domain is inherently interpretable for us, yet it resists adequate linguistic description. To unlock the full potential of this technology, we must push beyond the current paradigm dominated by large language models. I view this as an exciting research direction.
I started this line of work between my PhD at ETH Zurich and my EPFL postdoc, when I volunteered as an open-source contributor at LAION with Christoph Schuhmann, who connected me with the applied team at Stability AI where I worked as a contractor. Together with LAION contributors and Stability AI, I built large-scale image-text datasets with OCR annotations to teach SDXL to render text. One of the datasets we created, RenderedText [2] (12 million pieces of paper containing text with character-level annotations rendered into 3D environments using Blender), has not only helped SDXL learn how to render text but also been used to train vision-language models such as DeepSeek-VL2 [3] and Idefics [4]. 
After my contractor role, I kept collaborating with Peter Baylies, who mentored me at Stability AI, and I trained latent CLIP: a CLIP model capable of processing SDXL's latent images without the need of decoding them to images [5]. Training a ViT-B sized latent CLIP, represents my biggest pretraining run so far, leveraging open-clip’s DDP trainer on 128 NVIDIA A100 GPUs for 4 days and 9.5 hours. In order to create the dataset of captioned latent images required, I precompute latent images for more than 2 billion images in the LAION 2B dataset under 4 different data augmentations, resulting in a dataset of about 10 billion captioned latent images. I only briefly had access to this number of GPUs, while the Stability AI open-source contributors cluster still existed and performed all of these steps on my own time between my postdoc at EPFL and my contractor role at Stability. Since then, I view this direction as truly mine. I consistently and independently kept pushing this direction further. I am very proud of what we accomplished.
Vision. Another particularly exciting aspect of world model research is the potential to create interactive experiences that benefit humanity in ways that are hard to imagine today. I imagine breakthroughs in this area will transform a broad range of fields simultaneously, similar to how chat models today. 
One example I often return to is psychotherapy. Interestingly, VR-based therapy has already begun to show promise in the treatment of schizophrenia. I first learned about this through my brother, Hannes Wendler, a doctoral student working at the intersection of philosophy and psychology, during a visit to his conference in Copenhagen (ICNAP 2023). There, Julie Nordgaard presented remarkable work from her lab on developing therapies that use 3D avatars to represent the voices perceived by patients with schizophrenia. By combining these avatars with voice modulation techniques, therapists can temporarily take on the roles of the voices the patients hear. This approach helps shift the power dynamics between patients and their perceived voices, gradually transforming negative experiences into empowering ones. Beyond direct therapy, these avatars also help families better understand and empathize with the patients’ experiences. However, creating such therapeutic environments is currently a manual, labor-intensive process that requires close collaboration between technical experts and clinicians over extended periods. Advances in video and world modeling and in our ability to create, customize, and control interactive experiences, could directly alleviate this bottleneck. 
From a technical perspective, applications like this are especially fascinating because they clearly illustrate what is required to enable such experiences, e.g., support for multiple participants and tools facilitating the creation and real-time adaptation of such experiences. This setting also displays the pattern of an experience creator (the therapists) and multiple experience receivers (the patients and their families), each with distinct objectives, modes of engagement, and perspectives within the shared simulated world. This opens the door for creativity and innovations that may touch the multiple parts of world model creation. At the same time, it is obvious that fundamental challenges such as maintaining consistency, long-term coherence, spatial memory, realism, controllability, and scalability need to be overcome in order to achieve real world impact.
Why DeepMind. Google DeepMind’s research has been a constant source of inspiration throughout my entire journey as a deep learning researcher. From AlphaGo’s victory over Lee Sedol, to AlphaStar’s pro-level performance in StarCraft, to AlphaFold 2’s atomic-accuracy protein folding, to AlphaTensor’s acceleration of matrix multiplication, and most recently Genie 3’s glimpse into the future of real-time, high-fidelity world simulation. Each of these milestones has redefined what is possible using AI and how I see the world. 
Beyond that, DeepMind is uniquely positioned for conducting this type of research that is both computationally expensive and right at the boundary of human knowledge. It would be an honor to join the mothership and contribute to DeepMind’s mission of building AI that advances science and benefits humanity.
Best,

Chris Wendler
[1] One-Step is Enough: Sparse Autoencoders for Text-to-Image Diffusion Models; Viacheslav Surkov*, Chris Wendler*, Antonio Mari, Mikhail Terekhov, Justin Deschenaux, Robert West, Caglar Gulcehre, David Bau, Project page: https://sdxl-unbox.epfl.ch/; Paper: https://arxiv.org/abs/2410.22366 
[2] RenderedText; Chris Wendler, Gambot, LAION and Stability AI; https://huggingface.co/datasets/wendlerc/RenderedText
[3] Deepseek-VL2; Zhiyu Wu et al.; https://arxiv.org/abs/2412.10302
[4] Idefics; HuggingfaceM4; https://huggingface.co/HuggingFaceM4/idefics2-8b-base
[5] Controlling Latent Diffusion Using Latent CLIP; Jason Becker, Chris Wendler, Peter Baylies, Robert West, Christian Wressnegger; https://arxiv.org/abs/2503.08455


