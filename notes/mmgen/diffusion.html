<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>diffusion</title>
  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      line-height: 1.6;
      color: #333;
      max-width: 800px;
      margin: 0 auto;
      padding: 2em;
    }
    pre {
      background: #f6f8fa;
      padding: 1em;
      border-radius: 5px;
      overflow-x: auto;
      margin: 1.5em 0;
    }
    code {
      font-family: SFMono-Regular, Consolas, Liberation Mono, Menlo, monospace;
      background: #f6f8fa;
      padding: 0.2em 0.4em;
      border-radius: 3px;
      font-size: 85%;
    }
    pre code {
      background: none;
      padding: 0;
      font-size: 92%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.5em;
      margin-bottom: 0.5em;
      font-weight: 600;
      line-height: 1.25;
    }
    h1 { font-size: 2em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
    h2 { font-size: 1.5em; border-bottom: 1px solid #eaecef; padding-bottom: 0.3em; }
    p { margin: 1em 0; }
    a {
      color: #0366d6;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
    img {
      max-width: 100%;
      box-sizing: content-box;
    }
    table {
      border-collapse: collapse;
      width: 100%;
      margin: 1em 0;
    }
    table, th, td {
      border: 1px solid #dfe2e5;
    }
    th, td {
      padding: 6px 13px;
      text-align: left;
    }
    tr:nth-child(even) {
      background-color: #f6f8fa;
    }
    blockquote {
      margin: 0;
      padding: 0 1em;
      color: #6a737d;
      border-left: 0.25em solid #dfe2e5;
    }
    ul, ol {
      padding-left: 2em;
    }
    /* Syntax highlighting styles */
    .highlight .c, .highlight .cm, .highlight .c1, .highlight .cs { color: #6a737d; } /* Comment */
    .highlight .k, .highlight .kc, .highlight .kd, .highlight .kn, .highlight .kp, .highlight .kr, .highlight .kt { color: #d73a49; } /* Keyword */
    .highlight .s, .highlight .sb, .highlight .sc, .highlight .sd, .highlight .s2, .highlight .s1 { color: #032f62; } /* String */
    .highlight .na, .highlight .bp { color: #005cc5; } /* Name.Attribute, Name.Builtin.Pseudo */
    .highlight .nb, .highlight .nc, .highlight .nf { color: #6f42c1; } /* Name.Builtin, Name.Class, Name.Function */
    .highlight .no, .highlight .nd, .highlight .ni, .highlight .ne, .highlight .nv { color: #e36209; } /* Name.Constant, Name.Decorator, etc */
    .highlight .o, .highlight .ow { color: #d73a49; } /* Operator, Operator.Word */
    .highlight .m, .highlight .mf, .highlight .mh, .highlight .mi, .highlight .mo { color: #005cc5; } /* Literal.Number */
  </style>
  <!-- MathJax for rendering math equations -->
  <script type="text/javascript" id="MathJax-script" async
    src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
  </script>
</head>
<body>
  <h1 id="notation">Notation</h1>

<p>\(x_0\) … clean datapoint
\(x_T\) … (pure) noise
\(x_t\) … noised datapoint</p>

<h1 id="toy-example">Toy example</h1>

<p>\(x_0\) is a sample of r.v. \(X_0 \sim p_{\mbox{data}}(x)\) 
\(x_T\) is a sample of r.v. \(X_T \sim N(x; 0, 1)\); \(X_T\) and \(\epsilon\) can be used interchangably</p>

<p>Helpful assumption: \(Var(X_0) = Var(X_T) = 1\).</p>

<p>We can design \(X_t = a_t X_0 + b_t X_T\) such that \(Var(X_t) = 1\).</p>

<p>\(\mathbb{E}[X_t] = a_t E[X_0] + b_t\mathbb{E}[X_T] = a_t E[X_0]\).</p>

<p>\(Var(X_t) = E[X_t^2] - E[X_t]^2\) N.R. \((\mathbb{E}[(x - m)^2] = E[x^2] - 2E[x]E[x] + E[x]^2 = E[x^2] - E[x]^2\)</p>

\[E[X_t^2] - (a_t E[X_0])^2 = a_t^2 E[X_0^2] + b_t^2 - a_t^2 E[X_0]^2 = a_t^2 Var(X_0) + b_t^2 = a_t^2 + b_t^2\]

<p>NR. X_0 indep. X_T, thus, \(E[X_t^2] = E[a_t^2 X_0^2 + 2 a_t X_0 b_t X_T + b_t^2 X_T^2] = a_t^2 E[X_0^2] + 2 a_t b_t E[X_0] E[X_T] + b_t^2 Var(X_T) = a_t^2 E[X_0^2] + b_t^2 Var(X_T)\)</p>

<p>As a result, we get the constraint \(a_t^2 + b_t^2 = 1\) which we can use to link them together \(a_t = \sqrt{1 - b_t^2}\).</p>

<p>Since we want that \(X_T = \epsilon\) and \(X_0 = X_0\), \(b_0 = 0\) and \(b_T = 1\).</p>

<p>How is this different from the linear interpolation that we have in flow-matching?</p>

<p>\(x_t = x_T + (T - t) * (x_0 - x_T)\), with \(T = 1\) and \(t \in [0, 1]\)? –&gt; diffusion weights are non-linear in t.</p>

<h1 id="forward-process">Forward process</h1>

<p>The idea here is to obtain \(x_t\) from \(x_{t - 1}\) by adding a little bit of Gaussian noise, i.e., \(X_t = c_t X_{t - 1} + d_t \epsilon\). Again since we want \(Var(X_t) = 1\), we have \(d_t = \sqrt{1 - c_t^2}\). Okay and \(c_t^2\) here is typically called \(\alpha_t\).</p>

<p>Thus, we have \(X_t = \sqrt{\alpha_t} X_{t-1} + \sqrt{1 - \alpha_t} \epsilon\) which is distributed according to \(q(x_t \mid x_{t-1}) = N(x_t; \sqrt{\alpha_t} X_{t-1}, 1 - \alpha_t)\).</p>

<p>Successively applying gives: \(q(x_t \mid x_0)\)</p>

<p>E.g., consider \(X_2 = \sqrt{\alpha_1} X_1 + \sqrt{1 - \alpha_1} \epsilon = \sqrt{\alpha_1} (\sqrt{\alpha_0} X_0 + \sqrt{1  - \alpha_0} \epsilon) + \sqrt{1 - \alpha_1} \epsilon\)</p>

\[= \sqrt{\alpha_1} \sqrt{\alpha_0} X_0 + \sqrt{\alpha_1} \sqrt{1  - \alpha_0} \epsilon + \sqrt{1 - \alpha_1} \epsilon\]

<p>Okay so we have something that is distributed according to the sum of two Gaussians: 
NR.: \((*) = \sqrt{\alpha_1} \sqrt{1  - \alpha_0} \epsilon + \sqrt{1 - \alpha_1} \epsilon\)</p>

<p>\(N(x; 0, a) + N(x; 0, b) = N(x; 0, ?)\) if the two Gaussians are indpendent: we have N(x; 0, a + b)$$</p>

<p>Thus, in our case we have \(Var(*) = \alpha_1 (1 - \alpha_0) + (1 - \alpha_1) = \alpha_1 - \alpha_0 \alpha_1 + 1 - \alpha_1 = 1 - \alpha_0 \alpha_1 = 1 - \bar{\alpha}_2\), where \(\bar{\alpha}_t = \prod_{i = 0}^{t-1} \alpha_i\).</p>

<h2 id="setting-a-noise-schedule">Setting a noise schedule</h2>

<p>When setting different \(\bar{\alpha}_t\), we can compute \(\alpha_t\) using \(\alpha_t = \frac{\bar{\alpha}_{t+1}}{\bar{\alpha}_t}\)</p>

<p>So defining \(\bar{\alpha}_t\) for \(t=0, \dots, T\) we can train a model and subsequently sample it optimally using the corresponding \(\alpha_t\).</p>

<h1 id="backward-process">Backward process</h1>

<p>Let \(\epsilon_{\theta}(x_t, t)\) be our noise prediction.</p>

<p>Then based on rewriting \(x_t = \sqrt{\alpha_t} x_{t-1} +  \sqrt{1 - \alpha_t} \epsilon\) to 
\(\frac{x_t -  \sqrt{1 - \alpha_t} \epsilon}{\sqrt{\alpha_t}} = x_{t-1}\) we get our prediction rule:
\(x_{t-1} \approx \frac{x_t -  \sqrt{1 - \alpha_t} \epsilon_{\theta}(x_t, t)}{\sqrt{\alpha_t}}\)</p>

<h1 id="backward-process-gpt">Backward process (GPT)</h1>

<p>Let \(\varepsilon_{\theta}(x_t, t)\) be our model’s <strong>noise prediction</strong> at step \(t\).</p>

<hr />

<h2 id="true-posterior">True posterior</h2>

<p>The forward process<br />
\(q(x_t \mid x_{t-1}) = \mathcal{N}\big(x_t; \sqrt{\alpha_t} x_{t-1}, (1 - \alpha_t)I \big)\)
induces a true posterior
\(q(x_{t-1} \mid x_t, x_0) = \mathcal{N}\!\big(\mu_t(x_t, x_0), \tilde{\beta}_t I\big),\)
where
$$
\begin{aligned}
\mu_t(x_t, x_0)
&amp;= \frac{\sqrt{\bar{\alpha}_{t-1}}\beta_t}{1 - \bar{\alpha}_t}\, x_0</p>
<ul>
  <li>\frac{\sqrt{\alpha_t}(1 - \bar{\alpha}<em>{t-1})}{1 - \bar{\alpha}_t}\, x_t, <br />
\tilde{\beta}_t
&amp;= \beta_t \frac{1 - \bar{\alpha}</em>{t-1}}{1 - \bar{\alpha}_t}, \quad
\text{and } \beta_t = 1 - \alpha_t.
\end{aligned}
$$</li>
</ul>

<hr />

<h2 id="replacing-x_0-with-the-model-prediction">Replacing \(x_0\) with the model prediction</h2>

<p>Instead of conditioning on the true \(x_0\), we use the model’s noise prediction to estimate it:
\(\hat{x}_0(x_t, t)
= \frac{x_t - \sqrt{1 - \bar{\alpha}_t}\, \varepsilon_\theta(x_t, t)}{\sqrt{\bar{\alpha}_t}}.\)</p>

<p>Substituting \(\hat{x}_0\) into \(\mu_t(x_t, x_0)\) gives the <strong>sampling mean</strong>:
\(\boxed{
\mu_\theta(x_t, t)
= \frac{1}{\sqrt{\alpha_t}}
\left(
x_t - \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}\, \varepsilon_\theta(x_t, t)
\right).
}\)</p>

<hr />

<h2 id="sampling-rule">Sampling rule</h2>

<p>The backward sampling step is:
$$
x_{t-1}
= \mu_\theta(x_t, t)</p>
<ul>
  <li>\sqrt{\tilde{\beta}_t}\, z,
\quad z \sim \mathcal{N}(0, I).
$$</li>
</ul>

<hr />

<h2 id="deterministic-variant-ddim">Deterministic variant (DDIM)</h2>

<p>For deterministic sampling (DDIM), set \(\eta = 0\):
$$
x_{t-1}
= \mu_\theta(x_t, t)</p>
<ul>
  <li>\eta\, \sqrt{\tilde{\beta}_t}\, z,
\quad \eta = 0 \text{ (deterministic case)}.
$$</li>
</ul>

<hr />

<p><strong>Summary:</strong></p>
<ul>
  <li>The model predicts \(\varepsilon_\theta(x_t, t)\), an estimate of the marginal noise at time \(t\).</li>
  <li>The correct scaling in the backward mean uses \(\displaystyle \frac{\beta_t}{\sqrt{1 - \bar{\alpha}_t}}\), not \(\sqrt{1 - \alpha_t}\).</li>
  <li>\(\bar{\alpha}_t = \prod_{s=1}^t \alpha_s\) defines the cumulative signal level.</li>
</ul>


</body>
</html>
